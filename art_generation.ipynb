{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Deep Learning & Art: Neural Style Transfer\n",
        "- Implement the neural style transfer algorithm\n",
        "- Generate novel artistic images using your algorithm\n",
        "- Define the style cost function for Neural Style Transfer\n",
        "- Define the content cost function for Neural Style Transfer"
      ],
      "metadata": {
        "id": "Zt-fHxAavMjt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1 - Packages"
      ],
      "metadata": {
        "id": "-6bNeNEuvs3k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "import scipy.io\n",
        "import scipy.misc\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.pyplot import imshow\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import pprint\n",
        "from public_tests import *\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "o9fdcrRJvS3p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Neural Style Transfer (NST) is one of the most fun and interesting optimization techniques in deep learning. It merges two images, namely: a \"content\" image (C) and a \"style\" image (S), to create a \"generated\" image (G). The generated image G combines the \"content\" of the image C with the \"style\" of image S.\n",
        "\n",
        "Transfer Learning\n",
        "\n",
        "Neural Style Transfer (NST) uses a previously trained convolutional network, and builds on top of that. The idea of using a network trained on a different task and applying it to a new task is called transfer learning.\n",
        "\n",
        "Here we used the eponymously named VGG network from the original NST paper published by the Visual Geometry Group at University of Oxford in 2014. Specifically, we used VGG-19, a 19-layer version of the VGG network. This model has already been trained on the very large ImageNet database, and has learned to recognize a variety of low level features (at the shallower layers) and high level features (at the deeper layers).\n"
      ],
      "metadata": {
        "id": "VRwaqY1_vx-b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "  Load parameters from the VGG model"
      ],
      "metadata": {
        "id": "76xrMnYowXD7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tf.random.set_seed(272) # DO NOT CHANGE THIS VALUE\n",
        "pp = pprint.PrettyPrinter(indent=4)\n",
        "img_size = 400\n",
        "vgg = tf.keras.applications.VGG19(include_top=False,\n",
        "                                  input_shape=(img_size, img_size, 3),\n",
        "                                  weights='pretrained-model/vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5')\n",
        "\n",
        "vgg.trainable = False\n",
        "pp.pprint(vgg)"
      ],
      "metadata": {
        "id": "tdXs7sOuwdjx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " Neural Style Transfer (NST)\n",
        "Next, we build the Neural Style Transfer (NST) algorithm in three steps:\n",
        "\n",
        "First, we build the content cost function  ð½ð‘ð‘œð‘›ð‘¡ð‘’ð‘›ð‘¡(ð¶,ðº)\n",
        "\n",
        "Second, we build the style cost function  ð½ð‘ ð‘¡ð‘¦ð‘™ð‘’(ð‘†,ðº)\n",
        "\n",
        "Finally, put it all together to get  ð½(ðº)=ð›¼ð½ð‘ð‘œð‘›ð‘¡ð‘’ð‘›ð‘¡(ð¶,ðº)+ð›½ð½ð‘ ð‘¡ð‘¦ð‘™ð‘’(ð‘†,ðº)\n",
        " ."
      ],
      "metadata": {
        "id": "hT-CHCb-wreu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Computing the Content Cost***\n",
        "\n",
        "One goal you should aim for when performing NST is for the content in generated image G to match the content of image C\n",
        "* The shallower layers of a ConvNet tend to detect lower-level features such as edges and simple textures.\n",
        "* The deeper layers tend to detect higher-level features such as more complex textures and object classes.\n",
        "\n",
        " A method to achieve this is to calculate the content cost function, which will be defined as:\n",
        "\n",
        "ð½ð‘ð‘œð‘›ð‘¡ð‘’ð‘›ð‘¡(ð¶,ðº)=14Ã—ð‘›ð»Ã—ð‘›ð‘ŠÃ—ð‘›ð¶âˆ‘all entries(ð‘Ž(ð¶)âˆ’ð‘Ž(ðº))2(1)\n",
        "\n",
        "* Here,  ð‘›ð»,ð‘›ð‘Š\n",
        "  and  ð‘›ð¶\n",
        "  are the height, width and number of channels of the hidden layer you have chosen, and appear in a normalization term in the cost.\n",
        "* For clarity, note that  ð‘Ž(ð¶)\n",
        "  and  ð‘Ž(ðº)\n",
        "  are the 3D volumes corresponding to a hidden layer's activations.\n",
        "* In order to compute the cost  ð½ð‘ð‘œð‘›ð‘¡ð‘’ð‘›ð‘¡(ð¶,ðº)\n",
        " , it might also be convenient to unroll these 3D volumes into a 2D matrix, as shown below.\n",
        "* Technically this unrolling step isn't needed to compute  ð½ð‘ð‘œð‘›ð‘¡ð‘’ð‘›ð‘¡\n",
        " , but it will be good practice for when you do need to carry out a similar operation later for computing the style cost  ð½ð‘ ð‘¡ð‘¦ð‘™ð‘’\n",
        " ."
      ],
      "metadata": {
        "id": "u06qcJ1O2_-c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def compute_content_cost(content_output, generated_output):\n",
        "\n",
        "    a_C = content_output[-1]\n",
        "    a_G = generated_output[-1]\n",
        "\n",
        "\n",
        "\n",
        "    # Retrieve dimensions from a_G\n",
        "    m, n_H, n_W, n_C = a_G.get_shape().as_list()\n",
        "\n",
        "    # Reshape 'a_C' and 'a_G'\n",
        "    a_C_unrolled = tf.transpose(tf.reshape(a_C, shape=[m, -1, n_C]))\n",
        "    a_G_unrolled = tf.transpose(tf.reshape(a_G, shape=[m, -1, n_C]))\n",
        "\n",
        "    # compute the cost with tensorflow\n",
        "    J_content = tf.reduce_sum(tf.square(tf.subtract(a_C_unrolled, a_G_unrolled))) / (4 * n_H * n_W * n_C)\n",
        "\n",
        "    return J_content\n",
        "\n",
        "\n",
        "    \"\"\"\n",
        "    Computes the content cost\n",
        "\n",
        "    Arguments:\n",
        "    a_C -- tensor of dimension (1, n_H, n_W, n_C), hidden layer activations representing content of the image C\n",
        "    a_G -- tensor of dimension (1, n_H, n_W, n_C), hidden layer activations representing content of the image G\n",
        "\n",
        "    Returns:\n",
        "    J_content -- scalar that you compute using equation 1 above.\n",
        "    \"\"\""
      ],
      "metadata": {
        "id": "C0cjFnlDw-O8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* The content cost takes a hidden layer activation of the neural network, and measures how different  ð‘Ž(ð¶)\n",
        "  and  ð‘Ž(ðº)\n",
        "  are.\n",
        "* When you minimize the content cost later, this will help make sure  ðº\n",
        "  has similar content as  ð¶\n",
        " ."
      ],
      "metadata": {
        "id": "ESIDqSOe303S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Computing the Style Cost**"
      ],
      "metadata": {
        "id": "adJU2v5G36Vi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Style Matrix\n",
        "\n",
        "Gram matrix\n",
        "\n",
        "* The style matrix is also called a \"Gram matrix.\"\n",
        "* In linear algebra, the Gram matrix G of a set of vectors  (ð‘£1,â€¦,ð‘£ð‘›)\n",
        "  is the matrix of dot products, whose entries are  ðºð‘–ð‘—=ð‘£ð‘‡ð‘–ð‘£ð‘—=ð‘›ð‘.ð‘‘ð‘œð‘¡(ð‘£ð‘–,ð‘£ð‘—)\n",
        " .\n",
        "* In other words,  ðºð‘–ð‘—\n",
        "  compares how similar  ð‘£ð‘–\n",
        "  is to  ð‘£ð‘—\n",
        " : If they are highly similar, you would expect them to have a large dot product, and thus for  ðºð‘–ð‘—\n",
        "  to be large.\n",
        "\n",
        "***Two meanings of the variable  ðº***\n",
        "\n",
        "* Note that there is an unfortunate collision in the variable names used here. Following the common terminology used in the literature:\n",
        "   * ðº\n",
        "  is used to denote the Style matrix (or Gram matrix)\n",
        "   * ðº\n",
        "  also denotes the generated image.\n",
        "* For the sake of clarity, in this assignment  ðºð‘”ð‘Ÿð‘Žð‘š\n",
        "  will be used to refer to the Gram matrix, and  ðº\n",
        "  to denote the generated image."
      ],
      "metadata": {
        "id": "kezy4G9Z4Ba9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Gram Matrix\n",
        "def gram_matrix(A):\n",
        "    \"\"\"\n",
        "    Argument:\n",
        "    A -- matrix of shape (n_C, n_H*n_W)\n",
        "\n",
        "    Returns:\n",
        "    GA -- Gram matrix of A, of shape (n_C, n_C)\"\"\"\n",
        "    GA =tf.matmul(A, A, transpose_b = True)\n",
        "    return GA"
      ],
      "metadata": {
        "id": "sfCYNCqQxk9F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Layer style cost***"
      ],
      "metadata": {
        "id": "mrKzqRVA4onD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_layer_style_cost(a_S, a_G):\n",
        "\n",
        "    # Retrieve dimensions from a_G (â‰ˆ1 line)\n",
        "    _, n_H, n_W, n_C = a_G.get_shape().as_list()\n",
        "    # Reshape the tensors from (1, n_H, n_W, n_C) to (n_C, n_H * n_W) (â‰ˆ2 lines)\n",
        "    a_S = tf.transpose(tf.reshape(a_S, shape=[-1, n_C]))\n",
        "    a_G = tf.transpose(tf.reshape(a_G, shape=[-1, n_C]))\n",
        "\n",
        "\n",
        "    # Computing gram_matrices for both images S and G (â‰ˆ2 lines)\n",
        "    GS = gram_matrix(a_S)\n",
        "    GG = gram_matrix(a_G)\n",
        "\n",
        "    # Computing the loss (â‰ˆ1 line)\n",
        "    J_style_layer = tf.reduce_sum((GS-GG)**2) / (2*n_C*n_H*n_W)**2\n",
        "    return J_style_layer\n",
        "\n",
        "\n",
        "    \"\"\"\n",
        "    Arguments:\n",
        "    a_S -- tensor of dimension (1, n_H, n_W, n_C), hidden layer activations representing style of the image S\n",
        "    a_G -- tensor of dimension (1, n_H, n_W, n_C), hidden layer activations representing style of the image G\n",
        "\n",
        "    Returns:\n",
        "    J_style_layer -- tensor representing a scalar value, style cost defined above by equation (2)\n",
        "    \"\"\""
      ],
      "metadata": {
        "id": "cgkq_pq54wgA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "_SzWS6Nm5F1O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#choose layers to represent the style of the image and assign style costs:\n",
        "STYLE_LAYERS = [\n",
        "    ('block1_conv1', 0.2),\n",
        "    ('block2_conv1', 0.2),\n",
        "    ('block3_conv1', 0.2),\n",
        "    ('block4_conv1', 0.2),\n",
        "    ('block5_conv1', 0.2)]"
      ],
      "metadata": {
        "id": "Oi1iA0Ri5KOn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Description of compute_style_cost\n",
        "\n",
        "For each layer:\n",
        "\n",
        "* Select the activation (the output tensor) of the current layer.\n",
        "* Get the style of the style image \"S\" from the current layer.\n",
        "* Get the style of the generated image \"G\" from the current layer.\n",
        "* Compute the \"style cost\" for the current layer\n",
        "* Add the weighted style cost to the overall style cost (J_style)\n",
        "* Return the overall style cost."
      ],
      "metadata": {
        "id": "lPwb4EuS5UYK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def compute_style_cost(style_image_output, generated_image_output, STYLE_LAYERS=STYLE_LAYERS):\n",
        "\n",
        "\n",
        "    # initialize the overall style cost\n",
        "    J_style = 0\n",
        "\n",
        "    # Set a_S to be the hidden layer activation from the layer we have selected.\n",
        "    # The last element of the array contains the content layer image, which must not be used.\n",
        "    a_S = style_image_output[:-1]\n",
        "\n",
        "    # Set a_G to be the output of the choosen hidden layers.\n",
        "    # The last element of the list contains the content layer image which must not be used.\n",
        "    a_G = generated_image_output[:-1]\n",
        "    for i, weight in zip(range(len(a_S)), STYLE_LAYERS):\n",
        "        # Compute style_cost for the current layer\n",
        "        J_style_layer = compute_layer_style_cost(a_S[i], a_G[i])\n",
        "\n",
        "        # Add weight * J_style_layer of this layer to overall style cost\n",
        "        J_style += weight[1] * J_style_layer\n",
        "\n",
        "    return J_style\n",
        "\n",
        "\n",
        "    \"\"\"\n",
        "    Computes the overall style cost from several chosen layers\n",
        "\n",
        "    Arguments:\n",
        "    style_image_output -- our tensorflow model\n",
        "    generated_image_output --\n",
        "    STYLE_LAYERS -- A python list containing:\n",
        "                        - the names of the layers we would like to extract style from\n",
        "                        - a coefficient for each of them\n",
        "\n",
        "    Returns:\n",
        "    J_style -- tensor representing a scalar value, style cost defined above by equation (2)\n",
        "    \"\"\""
      ],
      "metadata": {
        "id": "AqIjCgUl5jpq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* The style of an image can be represented using the Gram matrix of a hidden layer's activations.\n",
        "* we get even better results by combining this representation from multiple different layers.\n",
        "* This is in contrast to the content representation, where usually using just a single hidden layer is sufficient.\n",
        "* Minimizing the style cost will cause the image  ðº\n",
        "  to follow the style of the image  ð‘†\n",
        " ."
      ],
      "metadata": {
        "id": "0P1T2XJx5sxu"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wonxxB_H57cN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Defining the Total Cost to Optimize***\n",
        "\n",
        "Finally,created a cost function that minimizes both the style and the content cost. The formula is:\n",
        "\n",
        "ð½(ðº)=ð›¼ð½_ð‘ð‘œð‘›ð‘¡ð‘’ð‘›ð‘¡(ð¶,ðº)+ð›½ð½_ð‘ ð‘¡ð‘¦ð‘™ð‘’(ð‘†,ðº)"
      ],
      "metadata": {
        "id": "ywWawQ6N2Zbf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Total cost\n",
        "@tf.function()\n",
        "def total_cost(J_content, J_style, alpha = 10, beta = 40):\n",
        "\n",
        "    J = alpha * J_content + beta * J_style\n",
        "\n",
        "    return J\n",
        "\n",
        "\"\"\"\n",
        "    Computes the total cost function\n",
        "\n",
        "    Arguments:\n",
        "    J_content -- content cost coded above\n",
        "    J_style -- style cost coded above\n",
        "    alpha -- hyperparameter weighting the importance of the content cost\n",
        "    beta -- hyperparameter weighting the importance of the style cost\n",
        "\n",
        "    Returns:\n",
        "    J -- total cost as defined by the formula above.\"\"\""
      ],
      "metadata": {
        "id": "5b87bgrdx3Wa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* The total cost is a linear combination of the content cost  ð½ð‘ð‘œð‘›ð‘¡ð‘’ð‘›ð‘¡(ð¶,ðº)\n",
        "  and the style cost  ð½ð‘ ð‘¡ð‘¦ð‘™ð‘’(ð‘†,ðº)\n",
        " .\n",
        "* ð›¼\n",
        "  and  ð›½\n",
        "  are hyperparameters that control the relative weighting between content and style."
      ],
      "metadata": {
        "id": "EbJ3iAb12Pxq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Solving the Optimization Problem\n",
        "\n",
        "1.   Load the content image\n",
        "2.   Load the style image\n",
        "3.   Randomly initialize the image to be generated\n",
        "4.   Load the VGG19 model\n",
        "5.   Compute the content cost\n",
        "6.   Compute the style cost\n",
        "7.   Compute the total cost\n",
        "8.   Define the optimizer and learning rate\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "5CDLkWikxZ3B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " Load the Content Image"
      ],
      "metadata": {
        "id": "Zqj8SiAGyWRe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "content_image = np.array(Image.open(\"images/louvre_small.jpg\").resize((img_size, img_size)))\n",
        "content_image = tf.constant(np.reshape(content_image, ((1,) + content_image.shape)))\n",
        "\n",
        "print(content_image.shape)\n",
        "imshow(content_image[0])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "85Ag6bcPxgXU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load the Style Image"
      ],
      "metadata": {
        "id": "mCjrFinHyVrJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "style_image =  np.array(Image.open(\"images/monet.jpg\").resize((img_size, img_size)))\n",
        "style_image = tf.constant(np.reshape(style_image, ((1,) + style_image.shape)))\n",
        "\n",
        "print(style_image.shape)\n",
        "imshow(style_image[0])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "wzomDKeQxf7G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Randomly Initialize the Image to be Generated**\n",
        "Now, we get to initialize the \"generated\" image as a noisy image created from the content_image.\n",
        "\n",
        "* The generated image is slightly\n",
        "correlated with the content image.\n",
        "* By initializing the pixels of the generated image to be mostly noise but slightly correlated with the content image, this will help the content of the \"generated\" image more rapidly match the content of the \"content\" image.\n",
        "\n"
      ],
      "metadata": {
        "id": "x4_PirUeykBi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "generated_image = tf.Variable(tf.image.convert_image_dtype(content_image, tf.float32))\n",
        "noise = tf.random.uniform(tf.shape(generated_image), -0.25, 0.25)\n",
        "generated_image = tf.add(generated_image, noise)\n",
        "generated_image = tf.clip_by_value(generated_image, clip_value_min=0.0, clip_value_max=1.0)\n",
        "\n",
        "print(generated_image.shape)\n",
        "imshow(generated_image.numpy()[0])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "jKwMlo6Dy5IL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Load Pre-trained VGG19 Model**"
      ],
      "metadata": {
        "id": "dvgVqKBwy8BO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_layer_outputs(vgg, layer_names):\n",
        "    \"\"\" Creates a vgg model that returns a list of intermediate output values.\"\"\"\n",
        "    outputs = [vgg.get_layer(layer[0]).output for layer in layer_names]\n",
        "\n",
        "    model = tf.keras.Model([vgg.input], outputs)\n",
        "    return model"
      ],
      "metadata": {
        "id": "hCJLSmGBzHwF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define the content layer and build the model\n",
        "content_layer = [('block5_conv4', 1)]\n",
        "\n",
        "vgg_model_outputs = get_layer_outputs(vgg, STYLE_LAYERS + content_layer)"
      ],
      "metadata": {
        "id": "vEvOtCLozKgQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#save the output for the content and style layer in separate variables\n",
        "content_target = vgg_model_outputs(content_image)  # Content encoder\n",
        "style_targets = vgg_model_outputs(style_image)     # Style encoder"
      ],
      "metadata": {
        "id": "kbsWt3oHzUMo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Compute Total Cost**\n",
        "Compute the Content image Encoding (a_C)"
      ],
      "metadata": {
        "id": "IrK03uvbzdZq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Assign the content image to be the input of the VGG model.\n",
        "# Set a_C to be the hidden layer activation from the layer we have selected\n",
        "preprocessed_content =  tf.Variable(tf.image.convert_image_dtype(content_image, tf.float32))\n",
        "a_C = vgg_model_outputs(preprocessed_content)"
      ],
      "metadata": {
        "id": "eLn5Kd61zzHP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compute the Style image Encoding (a_S)\n",
        "\n",
        "The code below sets a_S to be the tensor giving the hidden layer activation for STYLE_LAYERS using our style image."
      ],
      "metadata": {
        "id": "W3Uj3frVz1c2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Assign the input of the model to be the \"style\" image\n",
        "preprocessed_style =  tf.Variable(tf.image.convert_image_dtype(style_image, tf.float32))\n",
        "a_S = vgg_model_outputs(preprocessed_style)"
      ],
      "metadata": {
        "id": "zajIKALBz9Cu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " display the images generated by the style transfer model."
      ],
      "metadata": {
        "id": "SrRyOp_D0DqD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clip_0_1(image):\n",
        "    \"\"\"\n",
        "    Truncate all the pixels in the tensor to be between 0 and 1\n",
        "\n",
        "    Arguments:\n",
        "    image -- Tensor\n",
        "    J_style -- style cost coded above\n",
        "\n",
        "    Returns:\n",
        "    Tensor\n",
        "    \"\"\"\n",
        "    return tf.clip_by_value(image, clip_value_min=0.0, clip_value_max=1.0)\n",
        "\n",
        "def tensor_to_image(tensor):\n",
        "    \"\"\"\n",
        "    Converts the given tensor into a PIL image\n",
        "\n",
        "    Arguments:\n",
        "    tensor -- Tensor\n",
        "\n",
        "    Returns:\n",
        "    Image: A PIL image\n",
        "    \"\"\"\n",
        "    tensor = tensor * 255\n",
        "    tensor = np.array(tensor, dtype=np.uint8)\n",
        "    if np.ndim(tensor) > 3:\n",
        "        assert tensor.shape[0] == 1\n",
        "        tensor = tensor[0]\n",
        "    return Image.fromarray(tensor)"
      ],
      "metadata": {
        "id": "_imLc2rK0IUo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training the model\n",
        "\n",
        "Implement the train_step() function for transfer learning\n",
        "\n",
        "* Use the Adam optimizer to minimize the total cost J.\n",
        "* Use a learning rate of 0.01\n",
        "* Adam Optimizer documentation\n",
        "* Use tf.GradientTape to update the image. (Course 2 Week 3: TensorFlow Introduction Assignment)\n",
        "* Within the tf.GradientTape():\n",
        "\n",
        "1. Compute the encoding of the generated image using vgg_model_outputs. Assign the result to a_G.\n",
        "2. Compute the total cost J, using the global variables a_C, a_S and the local a_G\n",
        "3. Use alpha = 10 and beta = 40."
      ],
      "metadata": {
        "id": "MXwBpLBk0Uzw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
        "\n",
        "@tf.function()\n",
        "def train_step(generated_image):\n",
        "    with tf.GradientTape() as tape:\n",
        "        # In this function you must use the precomputed encoded images a_S and a_C\n",
        "\n",
        "        # Compute a_G as the vgg_model_outputs for the current generated image\n",
        "        a_G = vgg_model_outputs(generated_image)\n",
        "\n",
        "        # Compute the style cost\n",
        "        J_style = compute_style_cost(a_S,a_G)\n",
        "        # Compute the content cost\n",
        "        J_content = compute_content_cost(a_C, a_G)\n",
        "        # Compute the total cost\n",
        "        J = total_cost(J_content, J_style, 10, 40)\n",
        "\n",
        "    grad = tape.gradient(J, generated_image)\n",
        "\n",
        "    optimizer.apply_gradients([(grad, generated_image)])\n",
        "    generated_image.assign(clip_0_1(generated_image))\n",
        "    return J\n",
        "\n",
        "generated_image = tf.Variable(generated_image)\n",
        "\n",
        "train_step_test(train_step, generated_image)"
      ],
      "metadata": {
        "id": "VtWXG6A90xtm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 20000\n",
        "for i in range(epochs):\n",
        "    train_step(generated_image)\n",
        "    if i % 250 == 0:\n",
        "        print(f\"Epoch {i} \")\n",
        "    if i % 250 == 0:\n",
        "        image = tensor_to_image(generated_image)\n",
        "        imshow(image)\n",
        "        image.save(f\"output/image_{i}.jpg\")\n",
        "        plt.show()"
      ],
      "metadata": {
        "id": "msLotsM_1LsI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Show the 3 images in a row\n",
        "fig = plt.figure(figsize=(16, 4))\n",
        "ax = fig.add_subplot(1, 3, 1)\n",
        "imshow(content_image[0])\n",
        "ax.title.set_text('Content image')\n",
        "ax = fig.add_subplot(1, 3, 2)\n",
        "imshow(style_image[0])\n",
        "ax.title.set_text('Style image')\n",
        "ax = fig.add_subplot(1, 3, 3)\n",
        "imshow(generated_image[0])\n",
        "ax.title.set_text('Generated image')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Wge8tr8e1O9P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**References**\n",
        "\n",
        "The Neural Style Transfer algorithm was due to Gatys et al. (2015). Harish Narayanan and Github user \"log0\" also have highly readable write-ups this lab was inspired by. The pre-trained network used in this implementation is a VGG network, which is due to Simonyan and Zisserman (2015). Pre-trained weights were from the work of the MathConvNet team.\n",
        "\n",
        "\n",
        "* Leon A. Gatys, Alexander S. Ecker, Matthias Bethge, (2015). A Neural Algorithm of Artistic Style\n",
        "* Harish Narayanan, Convolutional neural networks for artistic style transfer.\n",
        "* Log0, TensorFlow Implementation of \"A Neural Algorithm of Artistic Style\".\n",
        "* Karen Simonyan and Andrew Zisserman (2015). Very deep convolutional networks for large-scale image recognition\n",
        "* MatConvNet."
      ],
      "metadata": {
        "id": "RwKx7G1Z1ZEM"
      }
    }
  ]
}